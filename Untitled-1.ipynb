{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SITE_URL = 'https://react.dev'\n",
    "BASE_URL = \"https://react.dev/learn\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def fetch_html(url):\n",
    "    try:\n",
    "        # Send a GET request to the URL\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()  # Raise an HTTPError for bad responses (4xx and 5xx)\n",
    "        \n",
    "        # Parse the HTML content using BeautifulSoup\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        return soup\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Failed to fetch HTML content from {url}: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_menu_items(base_url):\n",
    "    urls = []\n",
    "    response = requests.get(base_url)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    main_menu = soup.find_all('nav')\n",
    "\n",
    "\n",
    "    for menu in main_menu:\n",
    "        main_items = menu.find_all('a')\n",
    "\n",
    "        for main_item in main_items:\n",
    "            main_url = main_item['href'] if 'href' in main_item.attrs else None\n",
    "            if not main_url.startswith('http'):\n",
    "                main_url = BASE_URL + main_url\n",
    "\n",
    "            # Process sub-items\n",
    "            sub_items = []\n",
    "            sub_menu = main_item.find_next('ul')\n",
    "            if sub_menu:\n",
    "                sub_links = sub_menu.find_all('a')  # Sub-items\n",
    "                for sub_link in sub_links:\n",
    "                    sub_url = sub_link['href'] if 'href' in sub_link.attrs else None\n",
    "                    if sub_url and not sub_url.startswith('http'):\n",
    "                        URL = SITE_URL + sub_url\n",
    "                        urls.append(URL)\n",
    "\n",
    "    return list(set(urls))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_urls(section):\n",
    "    \"\"\"\n",
    "    Extracts and returns a list of URLs from the given HTML section.\n",
    "\n",
    "    Args:\n",
    "        section (bs4.element.Tag): A BeautifulSoup Tag object representing an HTML section.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of URLs (strings) extracted from the 'href' attributes of 'a' tags within the section.\n",
    "              If the section itself is an 'a' tag with an 'href' attribute, an empty list is returned.\n",
    "    \"\"\"\n",
    "    urls = []\n",
    "    if section.name == 'a' and section.has_attr('href'):\n",
    "        return []\n",
    "    for anchor in section.find_all('a', href=True):\n",
    "        url = anchor['href'].strip()\n",
    "        if not url.startswith('http'):\n",
    "            url = CURR_URL + url\n",
    "        urls.append(url)\n",
    "    return urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text(section, tags=['p', 'ul', 'ol']):\n",
    "    \"\"\"\n",
    "    Extract text from specified tags inside a section while preserving order.\n",
    "    Args:\n",
    "        section (bs4.element.Tag): The BeautifulSoup tag object representing the section to extract text from.\n",
    "        tags (list of str, optional): A list of tag names to extract text from. Defaults to ['p', 'ul', 'ol'].\n",
    "    Returns:\n",
    "        list of str: A list of strings containing the extracted text from the specified tags.\n",
    "    \"\"\"\n",
    "    \"\"\"Extract text from specified tags inside a section while preserving order.\"\"\"\n",
    "    content = []\n",
    "\n",
    "\n",
    "    if section.name in tags:\n",
    "        content.append(section.get_text(strip=True))\n",
    "    else:\n",
    "        # find all tags and scrap text\n",
    "        for element in section.find_all(True):\n",
    "            if element.name in tags:\n",
    "                content.append(element.get_text(strip=True))  # Extract text and remove extra spaces\n",
    "    \n",
    "    return content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_code(section):\n",
    "    \"\"\"\n",
    "    Extracts code blocks from the given section.\n",
    "\n",
    "    Args:\n",
    "        section (bs4.element.Tag): A BeautifulSoup Tag object representing an HTML section.\n",
    "\n",
    "    Returns:\n",
    "        str: A string containing the extracted code blocks.\n",
    "    \"\"\"\n",
    "    code_blocks = section.find_all(True, 'cm-line')\n",
    "    code_lines = []  \n",
    "    \n",
    "    for code in code_blocks:\n",
    "        line_text = ' '.join(span.text for span in code.find_all('span'))\n",
    "        \n",
    "        code_lines.append(line_text)\n",
    "    if code_lines:     \n",
    "        full_code = {\n",
    "            'language': 'javascript',\n",
    "            'code' :'\\n'.join(code_lines)\n",
    "            }\n",
    "        return full_code\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_section(header):\n",
    "    \"\"\"\n",
    "    Extracts content from HTML header sections (h1 to h3) recursively.\n",
    "    Args:\n",
    "        header (Tag): BeautifulSoup Tag object representing the header.\n",
    "    Returns:\n",
    "        dict: A dictionary containing the title, description, code samples, URLs, and subsections.\n",
    "    \"\"\"\n",
    "    title = header.text.strip()\n",
    "    subsections = []\n",
    "    text = []\n",
    "    codes = []\n",
    "    urls = []\n",
    "    \n",
    "    level = int(header.name[1]) \n",
    "    \n",
    "    for sibling in header.find_next_siblings():\n",
    "        # Stop when another header of the same or higher level is encountered\n",
    "        if sibling.name in ['h1', 'h2', 'h3'] and int(sibling.name[1]) <= level:\n",
    "            break\n",
    "        \n",
    "        # If the sibling is a header, Recursively process it as a subsection\n",
    "        if sibling.name in ['h1', 'h2', 'h3']:\n",
    "            subsections.append(extract_section(sibling)) \n",
    "        else:\n",
    "            # Otherwise, extract text and other relevant data\n",
    "            text.extend(extract_text(sibling)) \n",
    "            urls.extend(extract_urls(sibling)) \n",
    "            code_section = extract_code(sibling)\n",
    "            if code_section:\n",
    "                codes.append(code_section)\n",
    "    \n",
    "    content = \"\\n\".join(text)\n",
    "    return {\n",
    "        \"title\": title,\n",
    "        \"description\": content,\n",
    "        \"code_sample\": codes,\n",
    "        \"urls\": urls,\n",
    "        \"subsections\": subsections \n",
    "    }\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_page(main_url):\n",
    "    \"\"\"\n",
    "    Scrapes the given webpage URL and extracts the main title, summary, and sections.\n",
    "    Args:\n",
    "        main_url (str): The URL of the webpage to scrape.\n",
    "    Returns:\n",
    "        dict: A dictionary containing the page title, source, URL, and sections with summary and content.\n",
    "    \"\"\"\n",
    "    global CURR_URL\n",
    "    CURR_URL = main_url\n",
    "    soup = fetch_html(main_url)\n",
    "    if not soup:\n",
    "        return {}\n",
    "\n",
    "    main_title = soup.find('title').text.strip() if soup.find('title') else \"Untitled Page\"\n",
    "    sections = []\n",
    "\n",
    "    # get page summary before scraping for titles\n",
    "    first_h2 = soup.find('h2')\n",
    "    if first_h2:\n",
    "        texts = []\n",
    "        for sibling in first_h2.find_previous_siblings():\n",
    "            if sibling.name in ['h1', 'h3', 'h4']: \n",
    "                break\n",
    "            texts.extend(extract_text(sibling))\n",
    "        texts.reverse() \n",
    "        content = \"\\n\".join(texts)\n",
    "    for h2 in soup.find_all('h2'):\n",
    "        sections.append(extract_section(h2))\n",
    "\n",
    "    return {\n",
    "        \"title\": main_title,\n",
    "        \"source\": \"react\",\n",
    "        \"url\": main_url,\n",
    "        \"sections\": {\n",
    "            'summary': content, \n",
    "            'content': sections}\n",
    "    }\n",
    "   \n",
    "def scrape_react():\n",
    "    urls = extract_menu_items('https://react.dev/learn/your-first-component')\n",
    "    all_data = []\n",
    "    \n",
    "    for url in urls:\n",
    "        page_data = scrape_page(url)\n",
    "        if page_data:\n",
    "            all_data.append(page_data)\n",
    "    return all_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "def save_to_json(data, filename):\n",
    "    with open(filename, 'w') as json_file:\n",
    "        json.dump(data, json_file, indent=4)\n",
    "\n",
    "def scrape_and_save(save_file = 'scraped_data.json'):\n",
    "    react_docs = scrape_react()\n",
    "   \n",
    "    save_to_json(react_docs, 'react.json')\n",
    "    print(\"Saved To\", save_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Saved To scraped_data.json\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "scrape_and_save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Converted JSON to JSONL\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "def convert_json_to_jsonl(json_file, jsonl_file):\n",
    "    # Open and read the JSON file\n",
    "    with open(json_file, 'r') as file:\n",
    "        data = json.load(file)  # Load JSON content\n",
    "    \n",
    "    # Write to JSONL file\n",
    "    with open(jsonl_file, 'w') as file:\n",
    "        for record in data:\n",
    "            file.write(json.dumps(record) + '\\n')  # Convert each object to a JSON string and add newline\n",
    "\n",
    "# Example usage:\n",
    "convert_json_to_jsonl('react.json', 'scraped_data.jsonl')\n",
    "print(\"Converted JSON to JSONL\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
